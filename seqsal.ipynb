{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seqsal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyl4nm4rsh4ll/funsae/blob/master/seqsal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FMpaHjOoMaH"
      },
      "source": [
        "# ------------------------------------------------------------\n",
        "# \"THE BEERWARE LICENSE\" (Revision 42):\n",
        "# ------------------------------------------------------------\n",
        "# <dylan_marshall@fas.harvard.com>, <so@g.harvard.edu> and \n",
        "# <koo@cshl.edu> wrote this code. As long as you retain this\n",
        "# notice, you can do whatever you want with this stuff. If we \n",
        "# meet someday, and you think this stuff is worth it, you can\n",
        "# buy us a beer in return.\n",
        "# -Dylan Marshall, Sergey Ovchinnikov and Peter Koo\n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wICZdOirnzKH"
      },
      "source": [
        "#Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYDZjtHSgUm1"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import tensorflow.compat.v1.keras.backend as K1\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "tf1.disable_eager_execution()\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Activation, Dropout, BatchNormalization, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import Zeros, Constant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNLFMyug6hBb"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GquuwTSInWrp"
      },
      "source": [
        "#MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOEpLDorY4Et"
      },
      "source": [
        "def train_model(model, X, W, schedule, verbose=False):\n",
        "  N = X.shape[0]\n",
        "  idx = np.arange(N)\n",
        "  for batch_size, epochs in schedule:\n",
        "    if batch_size == N:\n",
        "      model.fit(X, X, sample_weight=W, batch_size=batch_size, verbose=verbose, epochs=epochs)\n",
        "    else:\n",
        "      for e in range(epochs):\n",
        "        np.random.shuffle(idx)\n",
        "        model.fit(X[idx], X[idx], sample_weight=W[idx], batch_size=batch_size, verbose=verbose, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hm4J4oCgW1Q"
      },
      "source": [
        "###MRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--mOOpgzgYj3"
      },
      "source": [
        "def mrf(X, W, use_bias=False, lam=0.01, train=True):\n",
        "  \n",
        "  N,L,A = X.shape\n",
        "  F = L*A\n",
        "  \n",
        "  # clear graph\n",
        "  tf1.reset_default_graph()\n",
        "  K.clear_session()\n",
        "    \n",
        "  #####################################################\n",
        "  # setup kernel\n",
        "  #####################################################\n",
        "  def cst_w(weights):\n",
        "    weights = (weights + K.transpose(weights)) / 2\n",
        "    mask = K.constant((1-np.eye(L))[:,None,:,None], dtype=tf.float32)\n",
        "    weights = K.reshape(weights, (L,A,L,A)) * mask\n",
        "    return K.reshape(weights,(F,F))\n",
        "  \n",
        "  params = {\"units\":F,\n",
        "            \"kernel_initializer\":Zeros,\n",
        "            \"kernel_regularizer\":l2((lam/N)*(L-1)*(A-1)/2),\n",
        "            \"kernel_constraint\":cst_w}\n",
        "  \n",
        "  #####################################################\n",
        "  # setup bias\n",
        "  #####################################################\n",
        "  if use_bias:\n",
        "    init_v = np.log((X.T*W).sum(-1).T + lam*np.log(W.sum()))\n",
        "    params[\"bias_initializer\"] = Constant(init_v - init_v.mean(-1, keepdims=True))\n",
        "    params[\"bias_regularizer\"] = l2(lam/N)\n",
        "  else:\n",
        "    params[\"use_bias\"] = False\n",
        "   \n",
        "  #####################################################\n",
        "  # setup model\n",
        "  #####################################################\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(L,A)))\n",
        "  model.add(Dense(**params))\n",
        "  model.add(Reshape((L, A)))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  \n",
        "  #####################################################\n",
        "  # compile and train\n",
        "  #####################################################\n",
        "  if train:\n",
        "    def loss(p, q):\n",
        "      return K.sum(K.categorical_crossentropy(p,q),-1)\n",
        "    model.compile(Adam(0.1*np.log(W.sum())/L), loss)\n",
        "    train_model(model,X,W,[[N,200]])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_yGEL9Dg82f"
      },
      "source": [
        "###LAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDCwz6Gng8Kn"
      },
      "source": [
        "# Linear Auto-Encoder\n",
        "def lae(X, W, \n",
        "        enc=[], rank=256, dec=[],\n",
        "        lam_w=0.1, lam_e=1.0, use_e=True,\n",
        "        use_bias=False, train=True):\n",
        "  \n",
        "  N,L,A = X.shape\n",
        "  F = L*A\n",
        "\n",
        "  # clear graph\n",
        "  tf1.reset_default_graph()\n",
        "  K.clear_session()\n",
        "    \n",
        "  # model params\n",
        "  params_w = {\"use_bias\":use_bias, \"kernel_regularizer\":l2(lam_w * F/N)}\n",
        "  params_e = {\"use_bias\":use_bias, \"kernel_regularizer\":l2(lam_e)}\n",
        "  \n",
        "  #####################################################\n",
        "  # encoder\n",
        "  #####################################################\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(L,A)))\n",
        "  for unit in enc: model.add(Dense(uni, **params_w))\n",
        "  model.add(Dense(rank, **params_w))\n",
        "  \n",
        "  #####################################################\n",
        "  # decoder\n",
        "  #####################################################\n",
        "  for unit in dec: model.add(Dense(uni, **params_w))\n",
        "  model.add(Dense(F, **params_w))\n",
        "  model.add(Reshape((L,A)))\n",
        "  if use_e: model.add(Dense(A, **params_e))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  \n",
        "  #####################################################\n",
        "  # compile and train\n",
        "  #####################################################\n",
        "  if train:\n",
        "    def loss(p, q):\n",
        "      return K.sum(K.categorical_crossentropy(p,q),-1)\n",
        "    \n",
        "    model.compile(Adam(0.1*np.log(W.sum())/L), loss)\n",
        "    train_model(model,X,W,[[32,5],[64,10],[128,20],[N,100]])  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dTbk3hbfn-O"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Blmq9GShrSq"
      },
      "source": [
        "def vae(X, W,\n",
        "        enc=[512, 512], rank=32, dec=[512, 512],\n",
        "        drop=0.5, lam=0.0, beta=0.5, train=True):\n",
        "  \n",
        "  N,L,A = X.shape\n",
        "  F = L * A\n",
        "  \n",
        "  # clear graph\n",
        "  tf1.reset_default_graph()\n",
        "  K.clear_session()\n",
        "    \n",
        "  # model params\n",
        "  params = {\"activation\":\"selu\"}\n",
        "  if lam > 0:\n",
        "    params[\"kernel_regularizer\"] = l2(lam * F/N)\n",
        "  \n",
        "  #####################################################\n",
        "  # encoder (E)\n",
        "  #####################################################\n",
        "  E_I = Input((L,A))\n",
        "  E = Flatten()(E_I)\n",
        "  for unit in enc:\n",
        "    E = Dense(unit, **params)(E)\n",
        "    E = Dropout(drop)(E)\n",
        "    E = BatchNormalization()(E)\n",
        "  \n",
        "  #####################################################\n",
        "  # latent (Z)\n",
        "  #####################################################\n",
        "  Z_mu = Dense(rank)(E)\n",
        "  Z_log_sg = Dense(rank)(E)\n",
        "  Z_sg = Lambda(lambda x: K.exp(0.5 * x))(Z_log_sg)\n",
        "  Z = Lambda(lambda x: x[0]+x[1]*K.random_normal(K.shape(x[0])))([Z_mu, Z_sg])\n",
        "  \n",
        "  model_EN    = Model(E_I, Z,    name=\"model_EN\")\n",
        "  model_EN_mu = Model(E_I, Z_mu, name=\"model_EN_mu\")\n",
        "  \n",
        "  #####################################################\n",
        "  # decoder (D)\n",
        "  #####################################################\n",
        "  D_I = Input((rank,))\n",
        "  D = D_I\n",
        "  for unit in dec:\n",
        "    D = Dense(unit, **params)(D)\n",
        "    D = Dropout(drop)(D)\n",
        "    D = BatchNormalization()(D)\n",
        "    \n",
        "  D = Dense(F, **params)(D)\n",
        "  D = Reshape((L,A))(D)\n",
        "  D_O = Activation(\"softmax\")(D)  \n",
        "  model_DE = Model(D_I, D_O, name=\"model_DE\")\n",
        "  \n",
        "  #####################################################\n",
        "  # autoencoder\n",
        "  #####################################################\n",
        "  model    = Model(E_I, model_DE(model_EN(E_I)))\n",
        "  model_mu = Model(E_I, model_DE(model_EN_mu(E_I)))\n",
        "\n",
        "  #####################################################\n",
        "  # compile and train\n",
        "  #####################################################\n",
        "  if train:  \n",
        "    def loss(p,q):\n",
        "      RE = K.sum(K.categorical_crossentropy(p,q),-1)\n",
        "      KL = beta*K.sum(K.square(Z_mu)+K.square(Z_sg)-Z_log_sg-1.0,-1)\n",
        "      return RE + KL\n",
        "    model.compile(\"adam\",loss)\n",
        "    train_model(model,X,W,[[64,50],[128,50],[256,50],[512,50],[1024,50],[2048,50]])\n",
        "        \n",
        "  return model_mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU7h1nsDnZFm"
      },
      "source": [
        "#OTHER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5Zqp1Znahd"
      },
      "source": [
        "def pw_saliency(model):\n",
        "  sess = K1.get_session()\n",
        "  i = K1.placeholder(shape=[],dtype=tf.int32)\n",
        "  out = model.output[:,i]\n",
        "  L,A = [int(s) for s in model.output.get_shape()[1:]]\n",
        "  sal = -K1.gradients(-K.sum(np.eye(A)*K.log(out + 1e-8)), model.input)[0]\n",
        "  null = np.zeros((A,L,A))\n",
        "  pw = np.array([sess.run(sal, {i:j, model.input:null}) for j in range(L)])\n",
        "  return 0.5*(pw+np.transpose(pw,(2,3,0,1)))\n",
        "\n",
        "def pw_contact_map(pw):\n",
        "  l2_norm = np.sqrt(np.square(pw[:,:20,:,:20]).sum((1,3)))\n",
        "  np.fill_diagonal(l2_norm, 0.0)\n",
        "  ap = l2_norm.sum(0)\n",
        "  ap = ap[None,:]*ap[:,None]/ap.sum()\n",
        "  l2_norm_apc = l2_norm - ap\n",
        "  np.fill_diagonal(l2_norm_apc, 0.0)\n",
        "  return l2_norm_apc\n",
        "\n",
        "def contact_auc(pred, meas, thresh=0.01):\n",
        "  eval_idx = np.triu_indices_from(meas, 6)\n",
        "  pred_, meas_ = pred[eval_idx], meas[eval_idx] \n",
        "  L = (np.linspace(0.1,1.0,10) * len(meas)).astype(\"int\")\n",
        "  sort_idx = np.argsort(pred_)[::-1]\n",
        "  return np.mean([(meas_[sort_idx[:l]] > thresh).mean() for l in L])\n",
        "\n",
        "def sco(p,q):\n",
        "  return (p * np.log(q + 1e-8)).sum((1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG0k-bNy9zOk"
      },
      "source": [
        "#DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGd5lobZxtmH",
        "outputId": "d4f9d662-f299-4475-ca4f-104b529eccad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "%%bash\n",
        "pip -q install gdown\n",
        "gdown https://drive.google.com/uc?id=1MqhiNhwgLNJ-rnD_YCzEniDYizEc2-YP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MqhiNhwgLNJ-rnD_YCzEniDYizEc2-YP\n",
            "To: /content/data.npy\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 17.2MB/s]\r27.3MB [00:00, 23.8MB/s]\r34.6MB [00:00, 27.6MB/s]\r59.8MB [00:00, 37.6MB/s]\r71.8MB [00:00, 42.5MB/s]\r86.0MB [00:00, 53.8MB/s]\r101MB [00:01, 52.3MB/s] \r111MB [00:01, 49.0MB/s]\r129MB [00:01, 62.4MB/s]\r141MB [00:01, 70.9MB/s]\r158MB [00:01, 86.1MB/s]\r174MB [00:01, 100MB/s] \r188MB [00:02, 87.3MB/s]\r213MB [00:02, 108MB/s] \r229MB [00:02, 78.6MB/s]\r252MB [00:02, 85.0MB/s]\r274MB [00:02, 104MB/s] \r288MB [00:03, 95.0MB/s]\r301MB [00:03, 97.3MB/s]\r313MB [00:03, 89.8MB/s]\r329MB [00:03, 103MB/s] \r344MB [00:03, 95.9MB/s]\r368MB [00:03, 116MB/s] \r383MB [00:03, 85.2MB/s]\r409MB [00:04, 107MB/s] \r426MB [00:04, 83.1MB/s]\r440MB [00:04, 95.5MB/s]\r454MB [00:04, 87.7MB/s]\r466MB [00:04, 77.4MB/s]\r488MB [00:04, 95.9MB/s]\r502MB [00:05, 87.6MB/s]\r524MB [00:05, 107MB/s] \r539MB [00:05, 84.7MB/s]\r563MB [00:05, 88.1MB/s]\r587MB [00:05, 109MB/s] \r602MB [00:06, 103MB/s]\r620MB [00:06, 117MB/s]\r628MB [00:06, 101MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uApbrEWByaao",
        "outputId": "7be77081-5349-4b9b-b550-6e84ad3882bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "[print(a, type(b)) for a, b in data.items()];"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X <class 'numpy.ndarray'>\n",
            "W <class 'numpy.ndarray'>\n",
            "dX <class 'numpy.ndarray'>\n",
            "dy <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPgn1af490iX"
      },
      "source": [
        "data = np.load(\"data.npy\",allow_pickle=True).item()\n",
        "X,dX = [np.eye(21)[data[k]] for k in (\"X\",\"dX\")]\n",
        "W,dY = data[\"W\"], data[\"dY\"]\n",
        "cons = data[\"cons\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKnLkQ04-oOA"
      },
      "source": [
        "#A = []\n",
        "#B = []\n",
        "o = 256\n",
        "for k in range(34,1000):\n",
        "  model = vae(X, W, rank=o)\n",
        "  loss = model.evaluate(X,X,sample_weight=W,verbose=False)\n",
        "  w = pw_saliency(model)\n",
        "  cons_pred = pw_contact_map(w)\n",
        "  cons_auc = contact_auc(cons_pred, cons)\n",
        "  dY_sco = sco(dX, model.predict(dX))\n",
        "  dY_pssm_sco = sco(dX, model.predict(X[0,None]))\n",
        "  A.append(dY_sco)\n",
        "  B.append(dY_pssm_sco)\n",
        "  print(o,k,loss,spearmanr(dY, dY_sco)[0], spearmanr(dY, dY_pssm_sco)[0], cons_auc, \n",
        "        spearmanr(dY, np.mean(A,0))[0], spearmanr(dY, np.mean(B,0))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax_GyaIhPp04",
        "outputId": "71e38118-1b5c-4f9b-bc0f-20605f0b5dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6520947988885377"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNsqEvEVQBTB",
        "outputId": "df1dd40c-69ad-4727-f5f8-287bd95c314a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7242218963189837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af_zaP-fOs5b",
        "outputId": "470c025c-ce74-46ff-c1da-4b7554ab9028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "vae_model = vae(X, W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-1541def38bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ff383cdd275f>\u001b[0m in \u001b[0;36mvae\u001b[0;34m(X, W, enc, rank, dec, drop, lam, beta, train)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# encoder (E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mE_I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLOb6wyPQVcO"
      },
      "source": [
        "vae_w = pw_saliency(mrf_model)\n",
        "vae_cons = pw_contact_map(vae_w)\n",
        "vae_cons_auc = contact_auc(vae_cons, cons)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7fdwY1bPAfX",
        "outputId": "760357f3-f240-41b5-8491-8960561c987d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "contact_auc(mrf_cons, cons)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8574528783672488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VbZU1sndAQ"
      },
      "source": [
        "#DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6zt-QZl9x37"
      },
      "source": [
        "# import internals\n",
        "import copy, itertools, json, os, pickle, sys, time\n",
        "# import externals\n",
        "from matplotlib import animation, cm, colors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as g\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import special\n",
        "from scipy import signal as sig\n",
        "from scipy.spatial.distance import jensenshannon, pdist, squareform, hamming\n",
        "import scipy.stats as stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glRrizW5bRl"
      },
      "source": [
        "def cce(p, q):\n",
        "  \"\"\"categorical cross entropy\"\"\"\n",
        "  return -np.sum(p * np.log(q + 1e-8), axis=(1, 2))\n",
        "\n",
        "def collate_dms(dms_data, wrt, considered=\"v_\", dms_info=[\n",
        "  \"mut\", \"x\", \"y\", \"ind\", \"pw\", \"v_μ\", \"v_1\", \"v_2\", \"v_3\", \"v_4\", \"v_5\"\n",
        "]):\n",
        "  \"\"\"clean DMS data for evaluation\n",
        "\n",
        "    dms_data := raw DMS data,\n",
        "    wrt := valid DMS data indices,\n",
        "    considered := DMS experiment contextualizing edge cases,\n",
        "    dms_info := reported DeepSeq data types\"\"\"\n",
        "\n",
        "  # clean s.t. viable mutants wrt MSA\n",
        "  dms_msa_pre = {\n",
        "    dms: {\n",
        "      mut: {\n",
        "        k: v for k, v in xy.items()\n",
        "      } for mut, xy in mut_xy.items() if xy[\"x\"] is not None\n",
        "    } for dms, mut_xy in dms_data.items()\n",
        "  }\n",
        "  # ensure edge cases (infs, nans) D.N.E.\n",
        "  return {\n",
        "    dms: {\n",
        "      v: np.stack([\n",
        "        val[v] for mut, val in dms_msa_pre[dms].items() if all([\n",
        "          np.isfinite(dms_msa_pre[col][mut][wrt])\n",
        "            for col in dms_data.keys() if considered in col\n",
        "        ])\n",
        "      ]) for v in dms_info\n",
        "    } for dms in dms_data.keys()\n",
        "  }\n",
        "\n",
        "def load_pkl(fname):\n",
        "  \"\"\"load pickled file\"\"\"\n",
        "  if \"pkl\" in fname:\n",
        "    with open(fname, \"rb\") as f: return pickle.load(f)\n",
        "  else: print(\"check file\"); return None\n",
        "\n",
        "def map_labels(l, _l2c):\n",
        "  \"\"\"sequence header phyla to color\"\"\"\n",
        "  return [_l2c[x] if x in _l2c.keys() else _l2c[\"else\"] for x in l]\n",
        "\n",
        "def sequence_identity(u, v):\n",
        "  \"\"\"calculate sequence identity for two sequences\"\"\"\n",
        "  lengths = set([len(u), len(v)])\n",
        "  assert len(lengths) == 1, print(\"hmmmm\")\n",
        "  length = list(lengths)[0]\n",
        "  U = np.array([_2_params[\"i2a\"][np.argmax(i)] for i in u])\n",
        "  V = np.array([_2_params[\"i2a\"][np.argmax(i)] for i in v])\n",
        "  idx = np.setdiff1d(\n",
        "    np.arange(length),\n",
        "    np.concatenate([np.where(U == \"-\")[0], np.where(V == \"-\")[0]])\n",
        "  )\n",
        "  return 1 - hamming(U[idx], V[idx])\n",
        "\n",
        "def split_data(msa, frac=0.1, numpy=True):\n",
        "  \"\"\"(training / validation) split\n",
        "\n",
        "    msa := multiple sequence alignment\n",
        "    frac := fraction training data assigned to validation,\n",
        "    numpy := file is numpy format\"\"\"\n",
        "  if numpy:\n",
        "    data = msa[\"clean\"]\n",
        "    # samples, length, amino acids\n",
        "    N, L, A = data.shape\n",
        "    # number partition samples, shuffled indices\n",
        "    n, shuff = int(N * frac), np.random.permutation(N)\n",
        "    indices = {\"train\": (n, N), \"valid\": (0, n)}\n",
        "    # train, valid\n",
        "    return {\n",
        "      part: {\n",
        "        \"x\": data[shuff[idx[0]:idx[1]]].reshape((-1, L, A)),\n",
        "        \"weights\": msa[\"weights\"][shuff[idx[0]:idx[1]]],\n",
        "        \"phyla\": msa[\"phyla\"][shuff[idx[0]:idx[1]]],\n",
        "        \"seq_id\": msa[\"seq_id\"][shuff[idx[0]:idx[1]]]\n",
        "      } for part, idx in indices.items()\n",
        "    }\n",
        "  # TODO\n",
        "  else: # file is HDF5\n",
        "    train_valid = np.array(data.get(\"train\")).astype(np.float32)\n",
        "    # number valid samples, shuffled indices\n",
        "    N, L, A = train_valid.shape\n",
        "    num, shuff = int(N * frac), np.random.permutation(N)\n",
        "    # train, valid\n",
        "    return {\n",
        "      \"train\": train_valid[shuff[num:]].reshape((-1, L, A)),\n",
        "      \"valid\": train_valid[shuff[:num]].reshape((-1, L, A)),\n",
        "      \"test\": np.array(data.get(\"test\")).astype(np.float32)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNpmUT6Vner8",
        "outputId": "7abdd882-c71d-4930-b0b5-b18e66021592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "%%bash\n",
        "pip -q install gdown\n",
        "gdown https://drive.google.com/uc?id=1RaH9ErtltosAEtKvokOBje2NCcye6oo3\n",
        "gdown https://drive.google.com/uc?id=1odSnJIjK95a_KsNfFnZ6x3NsNEye5qa1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RaH9ErtltosAEtKvokOBje2NCcye6oo3\n",
            "To: /content/beta_lactamase_P62593.pkl\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 34.9MB/s]\r8.91MB [00:00, 33.9MB/s]\r42.5MB [00:00, 45.5MB/s]\r67.6MB [00:00, 57.3MB/s]\r98.0MB [00:00, 75.7MB/s]\r119MB [00:00, 93.5MB/s] \r137MB [00:00, 107MB/s] \r154MB [00:01, 114MB/s]\r181MB [00:01, 137MB/s]\r202MB [00:01, 153MB/s]\r227MB [00:01, 144MB/s]\r256MB [00:01, 169MB/s]\r287MB [00:01, 195MB/s]\r315MB [00:01, 215MB/s]\r340MB [00:01, 160MB/s]\r370MB [00:02, 185MB/s]\r395MB [00:02, 201MB/s]\r422MB [00:02, 217MB/s]\r446MB [00:02, 186MB/s]\r474MB [00:02, 206MB/s]\r504MB [00:02, 223MB/s]\r530MB [00:02, 233MB/s]\r555MB [00:02, 217MB/s]\r582MB [00:03, 231MB/s]\r607MB [00:03, 201MB/s]\r636MB [00:03, 221MB/s]\r660MB [00:03, 226MB/s]\r684MB [00:03, 218MB/s]\r707MB [00:03, 208MB/s]\r735MB [00:03, 223MB/s]\r763MB [00:03, 237MB/s]\r789MB [00:03, 242MB/s]\r816MB [00:04, 251MB/s]\r842MB [00:04, 241MB/s]\r869MB [00:04, 248MB/s]\r897MB [00:04, 256MB/s]\r911MB [00:04, 197MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1odSnJIjK95a_KsNfFnZ6x3NsNEye5qa1\n",
            "To: /content/beta_lactamase_P62593.DMS.pkl\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 37.2MB/s]\r8.91MB [00:00, 17.6MB/s]\r42.5MB [00:00, 24.5MB/s]\r56.6MB [00:00, 32.6MB/s]\r90.7MB [00:00, 44.7MB/s]\r112MB [00:01, 58.6MB/s] \r132MB [00:01, 70.7MB/s]\r161MB [00:01, 91.4MB/s]\r185MB [00:01, 90.3MB/s]\r208MB [00:01, 110MB/s] \r237MB [00:01, 135MB/s]\r259MB [00:01, 153MB/s]\r281MB [00:01, 165MB/s]\r305MB [00:02, 181MB/s]\r328MB [00:02, 156MB/s]\r355MB [00:02, 179MB/s]\r378MB [00:02, 179MB/s]\r405MB [00:02, 189MB/s]\r426MB [00:03, 113MB/s]\r445MB [00:03, 55.5MB/s]\r472MB [00:03, 72.8MB/s]\r489MB [00:04, 79.5MB/s]\r516MB [00:04, 101MB/s] \r546MB [00:04, 123MB/s]\r566MB [00:04, 137MB/s]\r596MB [00:04, 161MB/s]\r620MB [00:04, 178MB/s]\r643MB [00:04, 185MB/s]\r665MB [00:04, 194MB/s]\r689MB [00:04, 206MB/s]\r712MB [00:04, 197MB/s]\r733MB [00:05, 101MB/s]\r750MB [00:06, 43.5MB/s]\r762MB [00:06, 44.9MB/s]\r781MB [00:07, 43.0MB/s]\r797MB [00:07, 55.0MB/s]\r814MB [00:08, 29.2MB/s]\r841MB [00:08, 39.8MB/s]\r857MB [00:08, 51.3MB/s]\r871MB [00:09, 29.0MB/s]\r892MB [00:09, 39.1MB/s]\r906MB [00:10, 23.3MB/s]\r919MB [00:10, 30.5MB/s]\r929MB [00:11, 38.2MB/s]\r945MB [00:11, 49.6MB/s]\r974MB [00:11, 57.1MB/s]\r1.01GB [00:11, 76.0MB/s]\r1.03GB [00:11, 95.7MB/s]\r1.05GB [00:11, 88.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZr_B2IZ418J",
        "outputId": "2f720fc4-df46-447e-84cd-5f93ccc4cc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%bash\n",
        "wget -q -nc https://grigoryanlab.org/confind/confind-msl-bin.tar.gz\n",
        "gunzip confind-msl-bin.tar.gz\n",
        "tar -xvf confind-msl-bin.tar\n",
        "rm confind-msl-bin.tar\n",
        "wget -q -nc https://files.rcsb.org/view/1ERO.pdb\n",
        "./confind --p 1ERO.pdb --rLib ./rotlibs | grep \"contact\" > meas_con.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confind\n",
            "rotlibs/\n",
            "rotlibs/EBL.out\n",
            "rotlibs/BEBL.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmVFddg35-j4"
      },
      "source": [
        "################################################################################\n",
        "# define plotting / globals / settings\n",
        "plt.style.use(\"default\")\n",
        "  # primary\n",
        "_1_params = {\n",
        "  \"alphabet\": \"ARNDCQEGHILKMFPSTWYV-\",\n",
        "  # maps to \"alphabet\"\n",
        "  \"tri_alphabet\": [\n",
        "    \"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLN\", \"GLU\", \"GLY\",\n",
        "    \"HIS\", \"ILE\", \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\",\n",
        "    \"THR\", \"TRP\", \"TYR\", \"VAL\", \"GAP\"\n",
        "  ]\n",
        "}\n",
        "  # secondary\n",
        "_2_params = {\n",
        "  \"a2i\": {AA: i for i, AA in enumerate(_1_params[\"alphabet\"])},\n",
        "  \"aaa2a\": dict(zip(_1_params[\"tri_alphabet\"], _1_params[\"alphabet\"])),\n",
        "  \"i2a\": {i: AA for i, AA in enumerate(_1_params[\"alphabet\"])}\n",
        "}\n",
        "# define BLOSUM62\n",
        "b62_raw = {\n",
        "  \"aa\": np.array([\n",
        "    \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\",\n",
        "    \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\",\n",
        "    \"T\", \"W\", \"Y\", \"V\", \"B\", \"Z\", \"X\", \"-\"\n",
        "  ]),\n",
        "  \"log_odds\": \"\"\"\n",
        "    4 -1 -2 -2  0 -1 -1  0 -2 -1 -1 -1 -1 -2 -1  1  0 -3 -2  0 -2 -1  0 -4 \n",
        "    -1  5  0 -2 -3  1  0 -2  0 -3 -2  2 -1 -3 -2 -1 -1 -3 -2 -3 -1  0 -1 -4 \n",
        "    -2  0  6  1 -3  0  0  0  1 -3 -3  0 -2 -3 -2  1  0 -4 -2 -3  3  0 -1 -4 \n",
        "    -2 -2  1  6 -3  0  2 -1 -1 -3 -4 -1 -3 -3 -1  0 -1 -4 -3 -3  4  1 -1 -4 \n",
        "    0 -3 -3 -3  9 -3 -4 -3 -3 -1 -1 -3 -1 -2 -3 -1 -1 -2 -2 -1 -3 -3 -2 -4 \n",
        "    -1  1  0  0 -3  5  2 -2  0 -3 -2  1  0 -3 -1  0 -1 -2 -1 -2  0  3 -1 -4 \n",
        "    -1  0  0  2 -4  2  5 -2  0 -3 -3  1 -2 -3 -1  0 -1 -3 -2 -2  1  4 -1 -4 \n",
        "    0 -2  0 -1 -3 -2 -2  6 -2 -4 -4 -2 -3 -3 -2  0 -2 -2 -3 -3 -1 -2 -1 -4 \n",
        "    -2  0  1 -1 -3  0  0 -2  8 -3 -3 -1 -2 -1 -2 -1 -2 -2  2 -3  0  0 -1 -4 \n",
        "    -1 -3 -3 -3 -1 -3 -3 -4 -3  4  2 -3  1  0 -3 -2 -1 -3 -1  3 -3 -3 -1 -4 \n",
        "    -1 -2 -3 -4 -1 -2 -3 -4 -3  2  4 -2  2  0 -3 -2 -1 -2 -1  1 -4 -3 -1 -4 \n",
        "    -1  2  0 -1 -3  1  1 -2 -1 -3 -2  5 -1 -3 -1  0 -1 -3 -2 -2  0  1 -1 -4 \n",
        "    -1 -1 -2 -3 -1  0 -2 -3 -2  1  2 -1  5  0 -2 -1 -1 -1 -1  1 -3 -1 -1 -4 \n",
        "    -2 -3 -3 -3 -2 -3 -3 -3 -1  0  0 -3  0  6 -4 -2 -2  1  3 -1 -3 -3 -1 -4 \n",
        "    -1 -2 -2 -1 -3 -1 -1 -2 -2 -3 -3 -1 -2 -4  7 -1 -1 -4 -3 -2 -2 -1 -2 -4 \n",
        "    1 -1  1  0 -1  0  0  0 -1 -2 -2  0 -1 -2 -1  4  1 -3 -2 -2  0  0  0 -4 \n",
        "    0 -1  0 -1 -1 -1 -1 -2 -2 -1 -1 -1 -1 -2 -1  1  5 -2 -2  0 -1 -1  0 -4 \n",
        "    -3 -3 -4 -4 -2 -2 -3 -2 -2 -3 -2 -3 -1  1 -4 -3 -2 11  2 -3 -4 -3 -2 -4 \n",
        "    -2 -2 -2 -3 -2 -1 -2 -3  2 -1 -1 -2 -1  3 -3 -2 -2  2  7 -1 -3 -2 -1 -4 \n",
        "    0 -3 -3 -3 -1 -2 -2 -3 -3  3  1 -2  1 -1 -2 -2  0 -3 -1  4 -3 -2 -1 -4 \n",
        "    -2 -1  3  4 -3  0  1 -1  0 -3 -4  0 -3 -3 -2  0 -1 -4 -3 -3  4  1 -1 -4 \n",
        "    -1  0  0  1 -3  3  4 -2  0 -3 -3  1 -1 -3 -1  0 -1 -3 -2 -2  1  4 -1 -4 \n",
        "    0 -1 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2  0  0 -2 -1 -1 -1 -1 -1 -4 \n",
        "    -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4  1\n",
        "  \"\"\"\n",
        "}\n",
        "b62 = pd.DataFrame({\n",
        "  aa: row for aa, row in zip(b62_raw[\"aa\"], [row for row in np.array([\n",
        "    x for x in b62_raw[\"log_odds\"].replace(\"\\n\", \" \").split(\" \") if x != \"\"\n",
        "  ]).reshape((24, 24))])\n",
        "})[[c for c in b62_raw[\"aa\"] if c in _1_params[\"alphabet\"]]].iloc[[\n",
        "  i for i, c in enumerate(b62_raw[\"aa\"]) if c in _1_params[\"alphabet\"]\n",
        "]].astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur5HeOAr7Pdn"
      },
      "source": [
        "file_data = {\n",
        "  \"beta_lactamase_P62593\": {\n",
        "    \"msa\": \"https://drive.google.com/uc?id=1RaH9ErtltosAEtKvokOBje2NCcye6oo3\",\n",
        "    \"dms\": \"https://drive.google.com/uc?id=1odSnJIjK95a_KsNfFnZ6x3NsNEye5qa1\",\n",
        "    \"pdb\": \"1ERO.pdb\"\n",
        "  }\n",
        "  # TODO: modular\n",
        "}\n",
        "file_pref = \"beta_lactamase_P62593\"\n",
        "considered_dms = \"Ranganathan2015\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3_kolG25T5L",
        "outputId": "3f7af144-292b-49a4-aa16-6ce4405087e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%%time\n",
        "msa = file_data[file_pref][\"msa\"]\n",
        "dms = file_data[file_pref][\"dms\"]\n",
        "pdb = file_data[file_pref][\"pdb\"]\n",
        "load_data = {\n",
        "  \"msa\": load_pkl(file_pref + \".pkl\"),\n",
        "  \"dms\": collate_dms(\n",
        "    dms_data=load_pkl(file_pref + \".DMS.pkl\"),\n",
        "    wrt=\"y\",\n",
        "    considered=considered_dms\n",
        "  )\n",
        "}\n",
        "# measured contacts dataframe\n",
        "meas_con_df = pd.read_csv(\"meas_con.txt\", sep=\"\\t\", header=None, names=[\n",
        "  \"kind\", \"i\", \"j\", \"con\", \"aa_i\", \"aa_j\"\n",
        "])\n",
        "# initialize measured contact map data\n",
        "meas_con_data = {\n",
        "  \"con_i_a\": dict(zip(\n",
        "    np.array([int(ai[2:]) for ai in np.concatenate([\n",
        "      meas_con_df[\"i\"].values, [meas_con_df[\"j\"].values[-1]]\n",
        "    ])]),\n",
        "    np.array([_2_params[\"aaa2a\"][aaa] for aaa in np.concatenate([\n",
        "      meas_con_df[\"aa_i\"].values, [meas_con_df[\"aa_j\"].values[-1]]\n",
        "    ])])\n",
        "  ))\n",
        "}\n",
        "meas_con_data.update({\n",
        "  \"vals\": np.zeros((\n",
        "    np.max(np.array([int(x[2:]) for x in meas_con_df[\"j\"]])) + 1,\n",
        "    np.max(np.array([int(x[2:]) for x in meas_con_df[\"j\"]])) + 1\n",
        "  ))\n",
        "})\n",
        "# fill measured contact map data\n",
        "for row in meas_con_df.itertuples():\n",
        "  i, j = int(row[2][2:]), int(row[3][2:])\n",
        "  meas_con_data[\"vals\"][i, j] = row[4]\n",
        "  meas_con_data[\"vals\"][j, i] = row[4]\n",
        "# valid measured contact indices            \n",
        "v_con_idx = np.intersect1d(\n",
        "  np.where(np.sum(meas_con_data[\"vals\"], 0) > 0)[0],\n",
        "  np.where(np.sum(meas_con_data[\"vals\"], 0) > 0)[0]\n",
        ")                                  \n",
        "meas_con_data[\"vals\"] = meas_con_data[\"vals\"][v_con_idx, :][:, v_con_idx]\n",
        "# reference sequence, measured contact sequence\n",
        "raw_ref_seq = \"\".join([\n",
        "  _2_params[\"i2a\"][np.argmax(i)] for i in load_data[\"msa\"][\"raw\"][0]\n",
        "])\n",
        "meas_con_seq = \"\".join(list(meas_con_data[\"con_i_a\"].values()))\n",
        "# map measured contact sequence against raw reference sequence\n",
        "meas_con_idx = raw_ref_seq.find(meas_con_seq)\n",
        "# PDB validity\n",
        "assert meas_con_idx > 0, \"\\ncheck PDB file\\n\"\n",
        "# viable PDB indices\n",
        "meas_con_data.update({\n",
        "  \"good_idx\": np.array([\n",
        "    i for i, j in enumerate(np.arange(meas_con_idx, len(raw_ref_seq)))\n",
        "      if j in load_data[\"msa\"][\"non_gap\"]\n",
        "  ])\n",
        "})\n",
        "# clean namespace\n",
        "del msa, dms, v_con_idx, raw_ref_seq, meas_con_seq, meas_con_idx, row, i, j\n",
        "# examine MSA / DMS data\n",
        "[print(\"\\nload_data['msa']:\")] + [\n",
        "  print(\"  \", a, b.shape) for a, b in load_data[\"msa\"].items()]\n",
        "[print(\"\\nload_data['dms']:\")] + [\n",
        "  print(\"  \", a, b[\"x\"].shape) for a, b in load_data[\"dms\"].items()];\n",
        "[print(\"\\nmeas_con_data:\")] + [\n",
        "  print(\"  \", a, len(b)) for a, b in meas_con_data.items()];\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "load_data['msa']:\n",
            "   raw (10062, 286, 21)\n",
            "   non_gap (252,)\n",
            "   clean (10062, 252, 21)\n",
            "   weights (10062,)\n",
            "   phyla (10062,)\n",
            "   seq_id (10062,)\n",
            "\n",
            "load_data['dms']:\n",
            "   Ranganathan2015.1 (4769, 252, 21)\n",
            "   Ranganathan2015.2 (4769, 252, 21)\n",
            "   Ranganathan2015.μ (4769, 252, 21)\n",
            "   Palzkill2012 (4769, 252, 21)\n",
            "   Tenaillon2013 (971, 252, 21)\n",
            "   Ostermeier2014 (4575, 252, 21)\n",
            "\n",
            "meas_con_data:\n",
            "   con_i_a 263\n",
            "   vals 263\n",
            "   good_idx 252\n",
            "\n",
            "CPU times: user 2.46 s, sys: 1.64 s, total: 4.1 s\n",
            "Wall time: 4.11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBYVyrmp7G-H"
      },
      "source": [
        "# references: raw / clean\n",
        "r_raw = load_data[\"msa\"][\"raw\"][0][None, :]\n",
        "r_clean = load_data[\"msa\"][\"clean\"][0][None, :]\n",
        "\n",
        "# cleaned one-hot seqs, effective weights\n",
        "X, W = load_data[\"msa\"][\"clean\"], load_data[\"msa\"][\"weights\"]\n",
        "\n",
        "# DMS one-hot seqs\n",
        "dX = load_data[\"dms\"][considered_dms + \".μ\"][\"x\"]\n",
        "\n",
        "# DMS effect\n",
        "dy = load_data[\"dms\"][considered_dms + \".μ\"][\"y\"]\n",
        "\n",
        "# DeepSeq VAE prediction\n",
        "dDS = load_data[\"dms\"][considered_dms + \".μ\"][\"v_μ\"]\n",
        "\n",
        "# DeepSeq pairwise prediction\n",
        "dDS_pw = load_data[\"dms\"][considered_dms + \".μ\"][\"pw\"]\n",
        "\n",
        "# DeepSeq indices\n",
        "DS_idx = np.isfinite(load_data[\"dms\"][considered_dms + \".μ\"][\"v_μ\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_9m9WK70jF"
      },
      "source": [
        "data = {\"X\":X.argmax(-1), \"W\":W, \"dX\":dX.argmax(-1), \"dY\":dy, \"cons\":meas_con_data[\"vals\"][:,meas_con_data[\"good_idx\"]][meas_con_data[\"good_idx\"],:]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3UvLkPv8Jie"
      },
      "source": [
        "np.save(\"data.npy\",data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfKwcwtuDApm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}