{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyl4nm4rsh4ll/funsae/blob/master/data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QIjsAXLRWxA"
      },
      "source": [
        "### imports / settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QIbQwOfPRMM5",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# native libs\n",
        "import itertools, os, pickle, re, sys, time, urllib\n",
        "from functools import reduce\n",
        "from io import StringIO\n",
        "# external libs\n",
        "from ete3 import NCBITaxa\n",
        "ncbi = NCBITaxa()\n",
        "import h5py\n",
        "import matplotlib\n",
        "from matplotlib import cm\n",
        "from matplotlib import colors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import signal as sig\n",
        "from scipy import special\n",
        "import scipy.stats as stats\n",
        "from scipy.spatial.distance import jensenshannon, pdist, squareform, hamming\n",
        "# settings\n",
        "sns.set_style(\"ticks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0yXjzHDRMMv"
      },
      "source": [
        "# MSA generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNyGl4XoRWxB"
      },
      "source": [
        "**parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30WBHmVCRxYP"
      },
      "outputs": [],
      "source": [
        "# primary\n",
        "_1_params = {\n",
        "  \"alphabet\": \"ARNDCQEGHILKMFPSTWYV-\",\n",
        "  \"alignments_dir\": \"data/alignments/\",\n",
        "  \"collated_dir\": \"data/collated/\",\n",
        "  \"DMS_dir\": \"data/DMS/\",\n",
        "  \"fig_dir\": \"figures/\",\n",
        "  \"pk_data_dir\": \"data/peter_koo/50_synthetic_40/\",\n",
        "  \"deep_seq_supp\": \"data/deep_seq_supp/\",\n",
        "  \"predictions\": \"results/predictions/\",\n",
        "  \"weights_EZ\": \"results/weights/EZ/\",\n",
        "  \"weights_EZD\": \"results/weights/EZD/\"\n",
        "}\n",
        "# secondary\n",
        "_2_params = {\n",
        "  \"a2i\": {AA: i for i, AA in enumerate(_1_params[\"alphabet\"])},\n",
        "  \"i2a\": {i: AA for i, AA in enumerate(_1_params[\"alphabet\"])}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnetF3bnRWxC"
      },
      "source": [
        "**methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "p1X7UbZrRWxC"
      },
      "outputs": [],
      "source": [
        "def collate_dms(dms_data, wrt, considered=\"v_\", dms_info=[\n",
        "  \"mut\", \"x\", \"y\", \"ind\", \"pw\", \"v_μ\", \"v_1\", \"v_2\", \"v_3\", \"v_4\", \"v_5\"\n",
        "]):\n",
        "  \"\"\"clean DMS data for evaluation\n",
        "    dms_data := raw DMS data,\n",
        "    wrt := reported DeepSeq data ...,\n",
        "    considered := DMS experiment contextualizing edge cases,\n",
        "    dms_info := reported DeepSeq data types\"\"\"\n",
        "\n",
        "  # clean s.t. viable mutants wrt MSA\n",
        "  dms_msa_pre = {\n",
        "    dms: {\n",
        "      mut: {\n",
        "        k: v for k, v in xy.items()\n",
        "      } for mut, xy in mut_xy.items() if xy[\"x\"] is not None\n",
        "    } for dms, mut_xy in dms_data.items()\n",
        "  }\n",
        "  # ensure edge cases (infs, nans) D.N.E.\n",
        "  return {\n",
        "    dms: {\n",
        "      v: np.stack([\n",
        "        val[v] for mut, val in dms_msa_pre[dms].items() if all([\n",
        "          np.isfinite(dms_msa_pre[col][mut][wrt])\n",
        "            for col in dms_data.keys() if considered in col\n",
        "        ])\n",
        "      ]) for v in dms_info\n",
        "    } for dms in dms_data.keys()\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3K5sIkfRMNR",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def clean_alignment(f_pref, f_suff, ret=False, num=0, aa=_1_params[\"alphabet\"]):\n",
        "  \"\"\"prepare alignment file for HHblits := canonical / uppercase AA, periods --> hyphens\n",
        "    f_pref / f_suff := alignment file prefix / suffix,\n",
        "    ret := return for debugging purposes,\n",
        "    num := initialize counter for number of sequences in alignment file,\n",
        "    aa := considered amino acids\"\"\"\n",
        "  # read, clean\n",
        "  with open(f_pref + f_suff, \"r\") as f_i:\n",
        "    parsed = {\"head\": [], \"seqs\": []}\n",
        "    for line in f_i.readlines():\n",
        "      # header\n",
        "      if line.startswith(\">\"):\n",
        "        num += 1\n",
        "        parsed[\"head\"].append(line)\n",
        "        parsed[\"seqs\"].append([])\n",
        "      # sequence\n",
        "      else: parsed[\"seqs\"][-1].append(line)\n",
        "  # join subsequences\n",
        "  parsed[\"seqs\"] = [\"\".join(seq) for seq in parsed[\"seqs\"]]\n",
        "  # clean sequences\n",
        "  cleaned_seqs = [{\n",
        "    i: x for i, x in enumerate(seq) if (x == x.upper() and x in aa)\n",
        "  } for seq in parsed[\"seqs\"]]\n",
        "  # update parsed\n",
        "  parsed[\"seqs\"] = [\"\".join(list(seq.values())) for seq in cleaned_seqs]\n",
        "  parsed.update({\"idx\": [list(seq.keys()) for seq in cleaned_seqs]})\n",
        "  # dominant length mode (expected)\n",
        "  exp_length = stats.mode(np.array([len(x) for x in parsed[\"seqs\"]]))[0][0]\n",
        "  # appropriately lengthed sequences\n",
        "  result = {\"head\": [], \"seqs\": [], \"idx\": []}\n",
        "  for i in range(num):\n",
        "    if len(parsed[\"seqs\"][i]) == exp_length:\n",
        "      result[\"head\"].append(parsed[\"head\"][i].rstrip())\n",
        "      result[\"seqs\"].append(parsed[\"seqs\"][i])\n",
        "      result[\"idx\"].append(parsed[\"idx\"][i])\n",
        "  # write\n",
        "  with open(f_pref + f_suff + \"_cleaned\", \"w\") as f_o:\n",
        "    f_o.write(\"\\n\".join([\n",
        "      \"\\n\".join([result[\"head\"][j], result[\"seqs\"][j]])\n",
        "        for j in range(len(result[\"head\"]))\n",
        "    ]))\n",
        "  # considered indices\n",
        "  if ret: return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "9VlrcUjiRWxD"
      },
      "outputs": [],
      "source": [
        "def get_phyla(head, ncbi=ncbi):\n",
        "  \"\"\"MSA header taxonomy ID to phyla\n",
        "    head := MSA headers\n",
        "    ncbi := NCBI Taxonomy dictionary\"\"\"\n",
        "\n",
        "  def _get_phyla(h):\n",
        "    \"\"\"map phylum to header (h)\"\"\"\n",
        "    try:\n",
        "      if \"OX=\" not in h: return \"other\"\n",
        "      taxa = h.split(\"OX=\")[1].split(\" \")[0]\n",
        "      return dict(zip(\n",
        "        ncbi.get_rank(ncbi.get_lineage(int(taxa))).values(),\n",
        "        ncbi.get_taxid_translator(ncbi.get_lineage(int(taxa))).values()\n",
        "      ))[\"phylum\"]\n",
        "    except: return \"other\"\n",
        "\n",
        "  # initialize\n",
        "  phyla = {head[0]: \"SOURCE\"}\n",
        "  phyla.update({h: _get_phyla(h) for h in head[1:]})\n",
        "  return list(phyla.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "R3pYpg83RWxD"
      },
      "outputs": [],
      "source": [
        "def load_pkl(fname):\n",
        "  \"\"\"load pickled file\"\"\"\n",
        "  if \"pkl\" in fname:\n",
        "    with open(fname, \"rb\") as f: return pickle.load(f)\n",
        "  else: print(\"check file\"); return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "DtK5DHDzRWxD"
      },
      "outputs": [],
      "source": [
        "def make_msa(seqs, thresh={\"gap\": 0.5, \"eff\": 0.8}, a2i=_2_params[\"a2i\"]):\n",
        "  \"\"\"converts list of sequences to multiple sequence alignment (MSA)\n",
        "    seqs := cleaned / filtered MSA sequences,\n",
        "    thresh := ignore MSA columns wrt gap / effective seq. weight wrt eff,\n",
        "    a2i := amino acid to alphabet index\"\"\"\n",
        "\n",
        "  def _check_AA(AA):\n",
        "    if AA.upper() not in a2i.keys(): return \"-\"\n",
        "    else: return AA.upper()\n",
        "\n",
        "  # raw msa\n",
        "  raw = np.array([[a2i[_check_AA(AA)] for AA in seq] for seq in seqs])  \n",
        "  # non-gapped columns wrt gap threshold\n",
        "  non_gap = np.where(np.mean((raw == a2i[\"-\"]).astype(np.float), 0) < thresh[\"gap\"])[0]\n",
        "  # raw msa, non-gapped columns, cleaned msa, sequence weights\n",
        "  return {\n",
        "    \"raw\": np.eye(len(a2i))[raw],\n",
        "    \"non_gap\": non_gap,\n",
        "    \"clean\": np.eye(len(a2i))[raw[:, non_gap]],\n",
        "    \"weights\": 1.0 / np.sum(1.0 * ((1.0 - squareform(pdist(raw, \"hamming\"))) >= thresh[\"eff\"]), -1)\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "qGQcZ5H3RWxD"
      },
      "outputs": [],
      "source": [
        "def make_mut_msa(ref, mut_info, non_gap, a2i=_2_params[\"a2i\"], cols=[\n",
        "  \"x\", \"y\", \"ind\", \"pw\", \"v_μ\", \"v_1\", \"v_2\", \"v_3\", \"v_4\", \"v_5\"\n",
        "]):\n",
        "  \"\"\"converts list of mutations to multiple sequence alignment (MSA)\n",
        "    ref := reference MSA, one-hot encoded,\n",
        "    mut_info := mutation information / measured & predicted,\n",
        "    non_gap := mapping of valid indices from raw MSA to cleaned MSA,\n",
        "    a2i := amino acid to integer,\n",
        "    cols := pre-ordained columns of < mut_info > keyword argument\"\"\"\n",
        "  \n",
        "  def _ref2mut(_ref, _mut):\n",
        "    \"\"\"create one-hot encoded mutant from reference\"\"\"\n",
        "    # decompose mutant amino acid context\n",
        "    _aa_idx, i, j = int(_mut[1:-1]) - 1, a2i[_mut[0]], a2i[_mut[-1]]\n",
        "    # valid MSA column\n",
        "    if _aa_idx not in non_gap: return None\n",
        "    elif i == j: return None\n",
        "    else: aa_idx = np.where(non_gap == _aa_idx)[0][0]\n",
        "    m = _ref.copy()\n",
        "    assert(m[aa_idx, i] == 1), f\"{_aa_idx}, {aa_idx}, {i}, {j}\"\n",
        "    assert(m[aa_idx, j] == 0), f\"{_aa_idx}, {aa_idx}, {i}, {j}\"\n",
        "    m[aa_idx, i], m[aa_idx, j] = 0, 1\n",
        "    return m\n",
        "\n",
        "  # info per mutation\n",
        "  return {\n",
        "    x: {\n",
        "      **{\"mut\": x, \"x\": _ref2mut(ref, x)},\n",
        "      **{i: float(j) for i, j in zip(cols[1:], [y, ind, pw, v_μ, v_1, v_2, v_3, v_4, v_5])}\n",
        "    } for x, y, ind, pw, v_μ, v_1, v_2, v_3, v_4, v_5\n",
        "      in zip(*[mut_info[col].values for col in cols])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "9TZww9FVRWxE"
      },
      "outputs": [],
      "source": [
        "def parse_fasta(fname):\n",
        "  \"\"\"disentangle FASTA headers and sequences\n",
        "    fname := filename\"\"\"\n",
        "  # open / define\n",
        "  lines = open(fname, \"r\")\n",
        "  parsed = {\"head\": [], \"seq\": []}\n",
        "  # parse\n",
        "  for line in lines:\n",
        "    line = line.rstrip()\n",
        "    try: \n",
        "      if line.startswith(\">\"):\n",
        "        parsed[\"head\"].append(line[1:])\n",
        "        parsed[\"seq\"].append([])\n",
        "      else: parsed[\"seq\"][-1].append(line)\n",
        "    except: continue\n",
        "  # close\n",
        "  lines.close()\n",
        "  # (headers, sequences)\n",
        "  return {\n",
        "    \"head\": np.array(parsed[\"head\"]),\n",
        "    \"seqs\": np.array([\"\".join(seq) for seq in parsed[\"seq\"]])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Kfs7UZIFRWxE"
      },
      "outputs": [],
      "source": [
        "def sequence_identity(u, v):\n",
        "  \"\"\"calculate sequence identity for two sequences\"\"\"\n",
        "  lengths = set([len(u), len(v)])\n",
        "  assert len(lengths) == 1, print(\"hmmmm\")\n",
        "  length = list(lengths)[0]\n",
        "  U = np.array([_2_params[\"i2a\"][np.argmax(i)] for i in u])\n",
        "  V = np.array([_2_params[\"i2a\"][np.argmax(i)] for i in v])\n",
        "  idx = np.setdiff1d(np.arange(length), np.concatenate([np.where(U == \"-\")[0], np.where(V == \"-\")[0]]))\n",
        "  return 1 - hamming(U[idx], V[idx])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsHWUThzRMOi"
      },
      "source": [
        "# Mutant Data Analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb6zD86bRWxE"
      },
      "outputs": [],
      "source": [
        "_supp_2 = pd.ExcelFile(\n",
        "  _1_params[\"deep_seq_supp\"] + \"supp_2_41592_2018_138_MOESM4_ESM.xlsx\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeZGljQpRWxE"
      },
      "source": [
        "**reference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOmITvLKRWxE"
      },
      "source": [
        "###### 0. search for homologous sequences\n",
        "\n",
        "run hhblits on protein family\n",
        "\n",
        "```bash\n",
        "/home/jupyter-dylan/HH_SUITE/hh3_4x/bin/hhblits -id 100 -cov 10 -diff 0 -noaddfilter -maxmem 80 -contxt /home/UNICLUST/hh-suite/data/context_data.crf -d /home/UNICLUST/uniclust30_2018_08/uniclust30_2018_08 -n 8 -e 1 -cpu 16 -o /dev/null -i \"protein alignment\".fasta -oa3m \"protein alignment\".a3m\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrjStfEVRWxE"
      },
      "outputs": [],
      "source": [
        "# 1. context, protein family\n",
        "file_suff = {\n",
        "  \"raw\": \"a3m\",\n",
        "  \"clean\": \"a3m_cleaned\",\n",
        "  \"filt\": \"a3m_cleaned_filt\",\n",
        "  \"meta_msa\": \"pkl\",\n",
        "  \"dms\": \"DMS.pkl\"\n",
        "}\n",
        "# define alignment file (DMS dir, DMS, DMS file prefix)\n",
        "file_pref = _1_params[\"DMS_dir\"] + \"beta_lactamase/new_BLAT.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVG8qnRWRWxE"
      },
      "outputs": [],
      "source": [
        "# 2. clean alignment file\n",
        "clean_alignment(file_pref, file_suff[\"raw\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSVCzwF_RWxE"
      },
      "source": [
        "###### 3. filter alignment file\n",
        "\n",
        "run hhfilter 80 % coverage thresholds\n",
        "```bash\n",
        "/home/jupyter-dylan/HH_SUITE/hh3_4x/bin/hhfilter -cov 80 -M a3m -i \"protein alignment\".a3m_cleaned -o \"protein alignment\".a3m_cleaned_filt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CJxP5qnRWxF"
      },
      "outputs": [],
      "source": [
        "# 4. headers / sequences from cleaned and filtered alignment file\n",
        "parsed = parse_fasta(file_pref + file_suff[\"filt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie4f2BOORWxF"
      },
      "outputs": [],
      "source": [
        "# 5. define MSA from parsed sequences\n",
        "meta_msa = make_msa(parsed[\"seqs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVZSTVv7RWxF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 6. assign phyla labels to MSA\n",
        "meta_msa.update({\"phyla\": np.array(get_phyla(parsed[\"head\"]))})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95SV4KHdRWxF"
      },
      "outputs": [],
      "source": [
        "# 7. sequence identity for each MSA sequence wrt reference sequence\n",
        "meta_msa.update({\"seq_id\": np.array([\n",
        "  sequence_identity(meta_msa[\"clean\"][0], x) for x in meta_msa[\"clean\"]\n",
        "])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOCjd1cmRWxF"
      },
      "outputs": [],
      "source": [
        "F, A = plt.subplots(figsize=(12, 4))\n",
        "A.hist(meta_msa[\"seq_id\"], log=True, bins=128)\n",
        "A.set_title(\"my new msa seq. id %\")\n",
        "A.set_xticks(np.linspace(0, 1, num=21)); A.set_xlim([0, 1])\n",
        "plt.tight_layout(); plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFBtT2NMRWxF"
      },
      "outputs": [],
      "source": [
        "[(a, b.shape) for a, b in meta_msa.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCyK9vsyRWxF"
      },
      "outputs": [],
      "source": [
        "F, A = plt.subplots(figsize=(12, 4))\n",
        "A.hist(meta_msa[\"seq_id\"], log=True, bins=128)\n",
        "A.set_title(\"my new msa seq. id %\")\n",
        "A.set_xticks(np.linspace(0, 1, num=21)); A.set_xlim([0, 1])\n",
        "plt.tight_layout(); plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzTz_OZpRWxF"
      },
      "outputs": [],
      "source": [
        "[(a, b.shape) for a, b in meta_msa.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPL_tz-ARWxF"
      },
      "outputs": [],
      "source": [
        "# 8. save\n",
        "with open(file_pref + file_suff[\"meta_msa\"], \"wb\") as f:\n",
        "  pickle.dump(meta_msa, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAwuX6mwRWxF"
      },
      "source": [
        "*Beta Lactamase, Ecoli*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GFyqkqORWxF"
      },
      "outputs": [],
      "source": [
        "# après. load\n",
        "meta_msa = load_pkl(file_pref + file_suff[\"meta_msa\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAvNEFMYRWxF"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "supp_2_blat = {\n",
        "  x: _supp_2.parse(x)\n",
        "    for x in _supp_2.sheet_names if x.startswith(\"BLAT\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DXlPFwpRWxF"
      },
      "outputs": [],
      "source": [
        "# check \n",
        "check_blat_dms = pd.DataFrame({\n",
        "  tag: supp_2_blat[\"BLAT_ECOLX_Ranganathan2015\"][col].values\n",
        "    for tag, col in {\n",
        "      \"meas_1\": \"2500_1\",\n",
        "      \"meas_2\": \"2500_2\",\n",
        "      \"meas_μ\": \"2500\",\n",
        "      \"ind\": \"mutation_effect_prediction_independent\",\n",
        "      \"pw\": \"mutation_effect_prediction_pairwise\",\n",
        "      \"DS_1\": \"mutation_effect_prediction_vae_1\",\n",
        "      \"DS_2\": \"mutation_effect_prediction_vae_2\",\n",
        "      \"DS_3\": \"mutation_effect_prediction_vae_3\",\n",
        "      \"DS_4\": \"mutation_effect_prediction_vae_4\",\n",
        "      \"DS_5\": \"mutation_effect_prediction_vae_5\",\n",
        "      \"DS_μ\": \"mutation_effect_prediction_vae_ensemble\"\n",
        "    }.items()\n",
        "})\n",
        "print(f\"DeepSeq for Ranganathan2015:\\n  {check_blat_dms.shape}\\n\")\n",
        "check_blat_dms.dropna().corr(\"spearman\").round(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cznoRGKGRWxF"
      },
      "source": [
        "# collate other DMS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyuaZ3f6RWxG"
      },
      "outputs": [],
      "source": [
        "# visualize correlations\n",
        "sns.pairplot(\n",
        "  check_blat_dms[[\"meas_μ\", \"ind\", \"pw\", \"DS_μ\"]].dropna(),\n",
        "  diag_kind=\"kde\", plot_kws={\"s\": 4, \"alpha\": 0.7}\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD4qWwq3RWxG"
      },
      "outputs": [],
      "source": [
        "# dms effects dataframes\n",
        "blat_dms_effects_dfs = {\n",
        "  dms.split(\"_\")[-1]: pd.DataFrame({\n",
        "    \"x\": supp_2_blat[dms.split(\".\")[0]][\"mutant\"],\n",
        "    \"y\": supp_2_blat[dms.split(\".\")[0]][effect],\n",
        "    \"ind\": supp_2_blat[dms.split(\".\")[0]][\"mutation_effect_prediction_independent\"],\n",
        "    \"pw\": supp_2_blat[dms.split(\".\")[0]][\"mutation_effect_prediction_pairwise\"],\n",
        "    \"v_μ\": supp_2_blat[dms.split(\".\")[0]][\"mutation_effect_prediction_vae_ensemble\"],\n",
        "    **{\"v_\" + str(i): supp_2_blat[dms.split(\".\")[0]][\"mutation_effect_prediction_vae_\" + str(i)]\n",
        "      for i in range(1, 6)}\n",
        "  }) for dms, effect in {\n",
        "    \"BLAT_ECOLX_Ranganathan2015.1\": \"2500_1\",\n",
        "    \"BLAT_ECOLX_Ranganathan2015.2\": \"2500_2\",\n",
        "    \"BLAT_ECOLX_Ranganathan2015.μ\": \"2500\",\n",
        "    \"BLAT_ECOLX_Palzkill2012\": \"ddG_stat\",\n",
        "    \"BLAT_ECOLX_Tenaillon2013\": \"MIC_score\",\n",
        "    \"BLAT_ECOLX_Ostermeier2014\": \"linear\"\n",
        "  }.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-JL_XTuRWxG"
      },
      "outputs": [],
      "source": [
        "# check viability\n",
        "ref_seq = parsed[\"seqs\"][0]\n",
        "print(\"ref_seq:\\n\" + ref_seq + \"\\n\\n\")\n",
        "for x, y in supp_2_blat.items():\n",
        "  z = [c for c, C in itertools.groupby([i[:-1] for i in y[\"mutant\"].values])]\n",
        "  z_aa = \"\".join([c[0] for c in z])\n",
        "  z_idx_diff = \",\".join(np.setdiff1d(\n",
        "    np.array([int(c[1:]) for c in z]),\n",
        "    meta_msa[\"non_gap\"]\n",
        "  ).astype(\"str\"))\n",
        "  z_idx_diff_inv = \",\".join(np.setdiff1d(\n",
        "    meta_msa[\"non_gap\"],\n",
        "    np.array([int(c[1:]) for c in z])\n",
        "  ).astype(\"str\"))\n",
        "  print(x + \"\\n\" + \"\\n\".join([\n",
        "    z_aa,\n",
        "    \"in DMS not in MSA ---> \" + z_idx_diff,\n",
        "    \"in MSA not in DMS ---> \" + z_idx_diff_inv\n",
        "  ]) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXlAyPJ-RWxG"
      },
      "outputs": [],
      "source": [
        "# create DMS MSA dictionary\n",
        "blat_dms_msa = {\n",
        "  dms: make_mut_msa(\n",
        "    ref=meta_msa[\"clean\"][0],\n",
        "    mut_info=mut_info,\n",
        "    non_gap=meta_msa[\"non_gap\"]\n",
        "  ) for dms, mut_info in blat_dms_effects_dfs.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTB0dI1bRWxG"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "with open(file_pref + file_suff[\"dms\"], \"wb\") as f:\n",
        "  pickle.dump(blat_dms_msa, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEAPACFtRWxG"
      },
      "source": [
        "# etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "8qp2t09SRWxG"
      },
      "outputs": [],
      "source": [
        "b62_raw = {\n",
        "  \"aa\": np.array([\n",
        "    \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\",\n",
        "    \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\",\n",
        "    \"T\", \"W\", \"Y\", \"V\", \"B\", \"Z\", \"X\", \"-\"\n",
        "  ]),\n",
        "  \"log_odds\": \"\"\"\n",
        "    4 -1 -2 -2  0 -1 -1  0 -2 -1 -1 -1 -1 -2 -1  1  0 -3 -2  0 -2 -1  0 -4 \n",
        "    -1  5  0 -2 -3  1  0 -2  0 -3 -2  2 -1 -3 -2 -1 -1 -3 -2 -3 -1  0 -1 -4 \n",
        "    -2  0  6  1 -3  0  0  0  1 -3 -3  0 -2 -3 -2  1  0 -4 -2 -3  3  0 -1 -4 \n",
        "    -2 -2  1  6 -3  0  2 -1 -1 -3 -4 -1 -3 -3 -1  0 -1 -4 -3 -3  4  1 -1 -4 \n",
        "    0 -3 -3 -3  9 -3 -4 -3 -3 -1 -1 -3 -1 -2 -3 -1 -1 -2 -2 -1 -3 -3 -2 -4 \n",
        "    -1  1  0  0 -3  5  2 -2  0 -3 -2  1  0 -3 -1  0 -1 -2 -1 -2  0  3 -1 -4 \n",
        "    -1  0  0  2 -4  2  5 -2  0 -3 -3  1 -2 -3 -1  0 -1 -3 -2 -2  1  4 -1 -4 \n",
        "    0 -2  0 -1 -3 -2 -2  6 -2 -4 -4 -2 -3 -3 -2  0 -2 -2 -3 -3 -1 -2 -1 -4 \n",
        "    -2  0  1 -1 -3  0  0 -2  8 -3 -3 -1 -2 -1 -2 -1 -2 -2  2 -3  0  0 -1 -4 \n",
        "    -1 -3 -3 -3 -1 -3 -3 -4 -3  4  2 -3  1  0 -3 -2 -1 -3 -1  3 -3 -3 -1 -4 \n",
        "    -1 -2 -3 -4 -1 -2 -3 -4 -3  2  4 -2  2  0 -3 -2 -1 -2 -1  1 -4 -3 -1 -4 \n",
        "    -1  2  0 -1 -3  1  1 -2 -1 -3 -2  5 -1 -3 -1  0 -1 -3 -2 -2  0  1 -1 -4 \n",
        "    -1 -1 -2 -3 -1  0 -2 -3 -2  1  2 -1  5  0 -2 -1 -1 -1 -1  1 -3 -1 -1 -4 \n",
        "    -2 -3 -3 -3 -2 -3 -3 -3 -1  0  0 -3  0  6 -4 -2 -2  1  3 -1 -3 -3 -1 -4 \n",
        "    -1 -2 -2 -1 -3 -1 -1 -2 -2 -3 -3 -1 -2 -4  7 -1 -1 -4 -3 -2 -2 -1 -2 -4 \n",
        "    1 -1  1  0 -1  0  0  0 -1 -2 -2  0 -1 -2 -1  4  1 -3 -2 -2  0  0  0 -4 \n",
        "    0 -1  0 -1 -1 -1 -1 -2 -2 -1 -1 -1 -1 -2 -1  1  5 -2 -2  0 -1 -1  0 -4 \n",
        "    -3 -3 -4 -4 -2 -2 -3 -2 -2 -3 -2 -3 -1  1 -4 -3 -2 11  2 -3 -4 -3 -2 -4 \n",
        "    -2 -2 -2 -3 -2 -1 -2 -3  2 -1 -1 -2 -1  3 -3 -2 -2  2  7 -1 -3 -2 -1 -4 \n",
        "    0 -3 -3 -3 -1 -2 -2 -3 -3  3  1 -2  1 -1 -2 -2  0 -3 -1  4 -3 -2 -1 -4 \n",
        "    -2 -1  3  4 -3  0  1 -1  0 -3 -4  0 -3 -3 -2  0 -1 -4 -3 -3  4  1 -1 -4 \n",
        "    -1  0  0  1 -3  3  4 -2  0 -3 -3  1 -1 -3 -1  0 -1 -3 -2 -2  1  4 -1 -4 \n",
        "    0 -1 -1 -1 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2  0  0 -2 -1 -1 -1 -1 -1 -4 \n",
        "    -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 -4  1\n",
        "  \"\"\"\n",
        "}\n",
        "b62 = pd.DataFrame({\n",
        "  aa: row for aa, row in zip(b62_raw[\"aa\"], [row for row in np.array([\n",
        "    x for x in b62_raw[\"log_odds\"].replace(\"\\n\", \" \").split(\" \") if x != \"\"\n",
        "  ]).reshape((24, 24))])\n",
        "})[[c for c in b62_raw[\"aa\"] if c in _1_params[\"alphabet\"]]].iloc[[\n",
        "  i for i, c in enumerate(b62_raw[\"aa\"]) if c in _1_params[\"alphabet\"]\n",
        "]].astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1PFok6QRWxI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYMeuhdBRWxI"
      },
      "source": [
        "### DeepSeq β-Lactamase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqJ-6Nl0RWxI"
      },
      "outputs": [],
      "source": [
        "ds = _1_params[\"DMS_dir\"] + \"beta_lactamase/BLAT_ECOLX_hmmerbit_plmc_n5_m30_f50_t0.2_r24-286_id100_b105.fas\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs3MrQBuRWxI"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyjRRNrzRWxI"
      },
      "outputs": [],
      "source": [
        "!head -n 10 data/DMS/beta_lactamase/BLAT_ECOLX_hmmerbit_plmc_n5_m30_f50_t0.2_r24-286_id100_b105.fas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgtL11OMRWxI"
      },
      "outputs": [],
      "source": [
        "def parse_ds(fname):\n",
        "  \"\"\"disentangle FASTA headers and sequences\n",
        "    fname := filename\"\"\"\n",
        "  # open / define\n",
        "  lines = open(fname, \"r\")\n",
        "  parsed = {\"head\": [], \"seq\": []}\n",
        "  # parse\n",
        "  for line in lines:\n",
        "    line = line.rstrip()\n",
        "    try: \n",
        "      if line.startswith(\">\"):\n",
        "        parsed[\"head\"].append(line[1:])\n",
        "        parsed[\"seq\"].append([])\n",
        "      else: parsed[\"seq\"][-1].append(line.upper())\n",
        "    except: continue\n",
        "  # close\n",
        "  lines.close()\n",
        "  # (headers, sequences)\n",
        "  return {\n",
        "    \"head\": np.array(parsed[\"head\"]),\n",
        "    \"seqs\": np.array([\"\".join(seq) for seq in parsed[\"seq\"]])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdbyIijbRWxI"
      },
      "outputs": [],
      "source": [
        "def make_msa_ds(seqs, thresholds={\"gap\": 0.5, \"eff\": 0.8}, a2i=_2_params[\"a2i\"]):\n",
        "  # raw msa\n",
        "  raw = np.array([[a2i[AA] if AA in a2i else a2i[\"-\"] for AA in seq] for seq in seqs])  \n",
        "  # non-gapped columns wrt gap threshold\n",
        "  non_gap = np.where(np.mean((\n",
        "    raw == a2i[\"-\"]).astype(np.float), 0\n",
        "  ) < thresholds[\"gap\"])[0]\n",
        "  # effective weights via inverse, normalized, Hamming distance\n",
        "  weights = 1.0 / np.sum((\n",
        "    (1.0 - squareform(pdist(raw, \"hamming\"))) >= thresholds[\"eff\"]\n",
        "  ).astype(np.float), -1)\n",
        "  # raw msa, non-gapped columns, cleaned msa, sequence weights\n",
        "  return {\n",
        "    \"raw\": np.eye(len(a2i))[raw],\n",
        "    \"non_gap\": non_gap,\n",
        "    \"clean\": np.eye(len(a2i))[raw[:, non_gap]],\n",
        "    \"weights\": weights\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LteGZN04RWxI"
      },
      "outputs": [],
      "source": [
        "parsed_ds = parse_ds(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fVkBnG4RWxI"
      },
      "outputs": [],
      "source": [
        "meta_msa_ds = make_msa_ds(parsed_ds[\"seqs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0BDx6-gRWxI"
      },
      "outputs": [],
      "source": [
        "b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgtzF569RWxI"
      },
      "outputs": [],
      "source": [
        "meta_msa_ds.update({\"seq_id\": np.array([\n",
        "  sequence_identity(meta_msa_ds[\"clean\"][0], x) for x in meta_msa_ds[\"clean\"]\n",
        "])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKvyZLKcRWxI"
      },
      "outputs": [],
      "source": [
        "meta_msa_ds[\"seq_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1_tiVFpRWxI"
      },
      "source": [
        "### New β-Lactamase a3m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93BgeUS2RWxI"
      },
      "outputs": [],
      "source": [
        "new = _1_params[\"DMS_dir\"] + \"beta_lactamase/new_beta_lactamase_P62593.a3m\"\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGrwYNKRWxI"
      },
      "outputs": [],
      "source": [
        "!head -n 5 data/DMS/beta_lactamase/new_beta_lactamase_P62593.a3m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y45KCFmbRWxI"
      },
      "outputs": [],
      "source": [
        "a = parse_fasta(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrxK8nS-RWxI"
      },
      "outputs": [],
      "source": [
        "a[\"seqs\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgWucARERWxI"
      },
      "outputs": [],
      "source": [
        "b = make_msa(a[\"seqs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ivgSmDRWxI"
      },
      "outputs": [],
      "source": [
        "def make_msa(seqs, thresholds={\"gap\": 0.5, \"eff\": 0.8}, a2i=_2_params[\"a2i\"]):\n",
        "  \"\"\"converts list of sequences to multiple sequence alignment (MSA)\n",
        "    seqs := cleaned / filtered MSA sequences,\n",
        "    thresholds := ignore MSA columns wrt gap / effective seq. weight wrt eff,\n",
        "    a2i := amino acid to alphabet index\"\"\"\n",
        "  \n",
        "  # raw msa\n",
        "  raw = np.array([[a2i[AA] if AA in a2i else a2i[\"-\"] for AA in seq] for seq in seqs])  \n",
        "  # non-gapped columns wrt gap threshold\n",
        "  non_gap = np.where(np.mean((\n",
        "    raw == a2i[\"-\"]).astype(np.float), 0\n",
        "  ) < thresholds[\"gap\"])[0]\n",
        "  # effective weights via inverse, normalized, Hamming distance\n",
        "  weights = 1.0 / np.sum((\n",
        "    (1.0 - squareform(pdist(raw, \"hamming\"))) >= thresholds[\"eff\"]\n",
        "  ).astype(np.float), -1)\n",
        "  # raw msa, non-gapped columns, cleaned msa, sequence weights\n",
        "  return {\n",
        "    \"raw\": np.eye(len(a2i))[raw],\n",
        "    \"non_gap\": non_gap,\n",
        "    \"clean\": np.eye(len(a2i))[raw[:, non_gap]],\n",
        "    \"weights\": weights\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sApIa4QgRWxI"
      },
      "outputs": [],
      "source": [
        "np.unique(np.array([len(x) for x in b]), return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvC9TutDRWxL"
      },
      "source": [
        "**--------------------------------------------------------------------------------------------------------------**\n",
        "# *_DEPRECATED_*\n",
        "**--------------------------------------------------------------------------------------------------------------**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7LCgY9LRWxL"
      },
      "source": [
        "## methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "foYVHVhBRWxL"
      },
      "outputs": [],
      "source": [
        "# def scrape_ids(x, head, ncbi=ncbi, batch_size=2000, head_split=\"/\", head_split_idx=0):\n",
        "#   \"\"\"taxonomy ID from UniProf ID, DeepSequence MSA headers\n",
        "#     x := MSA dataset name\n",
        "#     head := MSA dataset\n",
        "#     ncbi := NCBI Taxonomy dictionary\n",
        "#     batch_size := number headers scraped from UniProt\"\"\"\n",
        "\n",
        "#   def _get_phyla(taxa):\n",
        "#     \"\"\"map phylum\"\"\"\n",
        "#     try: return dict(zip(\n",
        "#       ncbi.get_rank(ncbi.get_lineage(int(taxa))).values(),\n",
        "#       ncbi.get_taxid_translator(ncbi.get_lineage(int(taxa))).values()\n",
        "#     ))[\"phylum\"]\n",
        "#     except: return \"other\"\n",
        "\n",
        "#   def _scrape(_uniprot_id):\n",
        "#     \"\"\"scrape info from UniProt\"\"\"\n",
        "#     try:\n",
        "#       scrape = str(urllib.request.urlopen(\n",
        "#         urllib.request.Request(\n",
        "#           \"https://www.uniprot.org/uploadlists/\",\n",
        "#           urllib.parse.urlencode({\n",
        "#             \"from\": \"NF100\", \"to\": \"NF100\",\n",
        "#             \"columns\": \"id,commontaxonid\", \"format\": \"tab\",\n",
        "#             \"query\": \" \".join(_uniprot_id),\n",
        "#           }).encode(\"utf-8\")\n",
        "#       )).read().decode(\"utf-8\"))\n",
        "#       # batch validity\n",
        "#       if len(scrape) > 0: return pd.read_csv(StringIO(scrape), sep=\"\\t\")\n",
        "#       else: print(\"invalid batch\"); return None\n",
        "#     # UniProt servers too weak\n",
        "#     except:\n",
        "#       print(\"bounced, waiting...\"); time.sleep(4)\n",
        "#       return _scrape(_uniprot_id)\n",
        "\n",
        "#   # initialize\n",
        "#   head = [x.split(head_split)[head_split_idx] for x in head]\n",
        "#   phyla = {head[0]: \"SOURCE\"}\n",
        "#   uniprot_id = np.unique([head[1:]])\n",
        "#   to_scrape = np.setdiff1d(uniprot_id, list(phyla.keys()))\n",
        "#   print(\"scraping:\", x)\n",
        "#   # collate\n",
        "#   while len(to_scrape) > 0:\n",
        "#     # UniRef100 ID\n",
        "#     batch = np.random.permutation(to_scrape)[:batch_size]\n",
        "#     # scrape info from UniRef100 ID\n",
        "#     info = _scrape(batch)\n",
        "#     if info is None: continue\n",
        "#     info = info[info[\"Common taxon ID\"].notnull()]\n",
        "#     # map UniRef100 to phylum\n",
        "#     phyla.update({\n",
        "#       cluster: _get_phyla(taxa) for cluster, taxa in dict(zip(\n",
        "#         info[\"Cluster ID\"].values,\n",
        "#         info[\"Common taxon ID\"].values\n",
        "#       )).items()\n",
        "#     })\n",
        "#     # update for all headers\n",
        "#     phyla.update({x: \"other\" for x in np.setdiff1d(\n",
        "#       batch, list(phyla.keys())\n",
        "#     )})\n",
        "#     # update remaining headers to be scraped\n",
        "#     to_scrape = np.setdiff1d(uniprot_id, list(phyla.keys()))\n",
        "#   print(\"  unique head, phyla:\", len(np.unique(head)), len(phyla))\n",
        "#   return phyla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apvv4yUxRWxL"
      },
      "source": [
        "*clean / filter data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No8WjkjGRMNj",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # clean\n",
        "# cleaned_a2ms = {\n",
        "#   x[:-4]: clean_a2m(_1_params[\"alignments_dir\"] + x)\n",
        "#     for x in os.listdir(_1_params[\"alignments_dir\"])\n",
        "#       if x.endswith(\".a2m\")\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A40OSqFoRMNm",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# # HHfilter\n",
        "# dir=\"data/alignments/\"\n",
        "# export HHLIB=hhsuite-2.0.16-linux-x86_64\n",
        "# for i in $(ls \"$dir\"); do\n",
        "#   if [[ $i == *\".a2m_cleaned\"* ]]; then\n",
        "#     hhsuite-2.0.16-linux-x86_64/bin/hhfilter -i \"$dir\"$i -id 99 -o \"$dir\"$i\"_filt\"\n",
        "#   fi\n",
        "# done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XARMudvRWxL"
      },
      "source": [
        "*create MSAs and MSA labels*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCry1Y8zRMNp",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # hhfiltered alignments\n",
        "# parsed_fastas = {\n",
        "#   y[1:]: {\n",
        "#     x.split(y)[0]: parse_fasta(_1_params[\"alignments_dir\"] + x)\n",
        "#       for x in os.listdir(_1_params[\"alignments_dir\"]) if x.endswith(y)\n",
        "#   } for y in [\".a2m\", \".a2m_cleaned\", \".a2m_cleaned_filt\"]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IwhbpuVRMNt",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # define MSA\n",
        "# meta_msa = {\n",
        "#   x: make_msa(parsed_fastas[\"a2m_cleaned_filt\"][x][\"seqs\"])\n",
        "#     for x in parsed_fastas[\"a2m_cleaned_filt\"].keys()\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "KX7pOzHGRWxL"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # scrape taxa\n",
        "# meta_taxa = {\n",
        "#   x: scrape_ids(x, parsed_fastas[\"a2m_cleaned_filt\"][x][\"head\"])\n",
        "#     for x in parsed_fastas[\"a2m_cleaned_filt\"].keys()\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "X6VzB2diRWxL"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # map MSA headers to taxa labels\n",
        "# meta_labels = {\n",
        "#   x: {\n",
        "#     head: meta_taxa[x][head.split(\"/\")[0]]\n",
        "#       for head in parsed_fastas[\"a2m_cleaned_filt\"][x][\"head\"]\n",
        "#   } for x in parsed_fastas[\"a2m_cleaned_filt\"].keys()\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "JKW8RupvRWxL"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # map MSA headers with sequence indices remaining from original\n",
        "# meta_orig_idx = {\n",
        "#   x: {\n",
        "#     head: cleaned_a2ms[x][head]\n",
        "#       for head in parsed_fastas[\"a2m_cleaned_filt\"][x][\"head\"]\n",
        "#   } for x in parsed_fastas[\"a2m_cleaned_filt\"].keys()\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "i8HUWDEJRWxM"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # update meta_msa with labels\n",
        "# for x, y in meta_labels.items(): meta_msa[x].update({\"labels\": y})\n",
        "# # update meta_msa with original sequence indices\n",
        "# for x, y in meta_orig_idx.items(): meta_msa[x].update({\"idx\": y})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0ETQHp5RMN2",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# # save MSAs\n",
        "# for x in meta_msa.keys():\n",
        "#   fname = _1_params[\"collated_dir\"] + x + \".pkl\"\n",
        "#   print(\"pickling\", fname)\n",
        "#   with open(fname, \"wb\") as f:\n",
        "#     pickle.dump(meta_msa[x], f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha04qy0mRWxM"
      },
      "source": [
        "*mutant DMS*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "oeIjHUHkRWxM"
      },
      "outputs": [],
      "source": [
        "# supp_2 = {\n",
        "#   x: _supp_2.parse(x)#.dropna()\n",
        "#     for x in _supp_2.sheet_names# if x.startswith(\"BLAT\")\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "fA85UdZgRWxM"
      },
      "outputs": [],
      "source": [
        "# pkl = \"BLAT_ECOLX_1_b0.5.pkl\"\n",
        "# msa = load_pkl(_1_params[\"collated_dir\"] + pkl)\n",
        "# ref = {\n",
        "#   \"head\": list(msa[\"labels\"].keys())[0],\n",
        "#   \"seq\": msa[\"msa_clean\"][0]\n",
        "# }\n",
        "# # valid columns indices\n",
        "# assert all(\n",
        "#   np.average(list(msa[\"idx\"].values()), 0).astype(\"int\") ==\\\n",
        "#   msa[\"idx\"][ref[\"head\"]]\n",
        "# )\n",
        "# # valid indices accounting for offset\n",
        "# v_idx = np.array(msa[\"idx\"][ref[\"head\"]]) + int(ref[\"head\"].split(\"/\")[1].split(\"-\")[0])\n",
        "# # mutations, mutation effect\n",
        "# MUT = {\n",
        "#   \"muts\": supp_2[\"BLAT_ECOLX_Ranganathan2015\"][\"mutant\"].values,\n",
        "#   \"vals\": supp_2[\"BLAT_ECOLX_Ranganathan2015\"][\"2500\"].values\n",
        "# }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_prep.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}